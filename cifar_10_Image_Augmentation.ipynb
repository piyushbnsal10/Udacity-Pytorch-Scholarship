{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar 10 Image_Augmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushbnsal10/Udacity-Pytorch-Scholarship/blob/master/cifar_10_Image_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BLzS3Qi9qTGs",
        "colab_type": "code",
        "outputId": "43ed53f8-376e-497c-916a-b8524bbea3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "  print(\"Cuda is available\")\n",
        "else:\n",
        "  print(\"Cuda not is available\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qhb_854XtCuE",
        "colab_type": "code",
        "outputId": "da267f8b-20e1-4465-c0dc-7b669a94735c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "batch_size=20\n",
        "valid_size=0.2\n",
        "\n",
        "transform=transforms.Compose([transforms.RandomHorizontalFlip(),transforms.RandomRotation(10),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "\n",
        "train_data=datasets.CIFAR10('data',train=True,download=True,transform=transform)\n",
        "test_data=datasets.CIFAR10('data',train=False,download=True,transform=transform)\n",
        "\n",
        "num_train=len(train_data)\n",
        "indices=list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split=int(np.floor(valid_size*num_train))\n",
        "train_idx,valid_idx=indices[split:],indices[:split]\n",
        "\n",
        "train_sampler=SubsetRandomSampler(train_idx)\n",
        "valid_sampler=SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,sampler=train_sampler)\n",
        "valid_loader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,sampler=valid_sampler)\n",
        "test_loader=torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n",
        "\n",
        "classes=['Airplane','Automobile','Bird','Cat','Deer','Dog','Frog','Horse','Ship','Truck']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H61xaD2Myzrw",
        "colab_type": "code",
        "outputId": "938e1c82-02d8-4b1e-df73-8fd1661c900b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "data=train_data[15000][0].numpy()\n",
        "data=data*0.5+0.5\n",
        "data=np.transpose(data,(1,2,0))\n",
        "\n",
        "print(data.shape)\n",
        "plt.imshow(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc484fec470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD5CAYAAAAOeCiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "k7E1uRRTOnDX",
        "colab_type": "code",
        "outputId": "46b99d4b-c799-4445-d4fd-3b3be2421f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.conv1=nn.Conv2d(3,16,3,padding=1)\n",
        "    self.conv2=nn.Conv2d(16,32,3,padding=1)\n",
        "    self.conv3=nn.Conv2d(32,64,3,padding=1)\n",
        "    self.dropout=nn.Dropout(0.25)\n",
        "    self.pool=nn.MaxPool2d(2,2)\n",
        "    self.fc1=nn.Linear(64*4*4,500)\n",
        "    self.fc2=nn.Linear(500,10)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x=self.pool(F.relu(self.conv1(x)))\n",
        "    x=self.pool(F.relu(self.conv2(x)))\n",
        "    x=self.pool(F.relu(self.conv3(x)))\n",
        "  \n",
        "    x=x.view(-1,64*4*4)\n",
        "  \n",
        "    x=self.dropout(x)\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=self.dropout(x)\n",
        "    x=self.fc2(x)\n",
        "    return x\n",
        "\n",
        "model=Net()\n",
        "print(model)\n",
        "  \n",
        "if train_on_gpu:\n",
        "  model.cuda()\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (dropout): Dropout(p=0.25)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
            "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gWbSU3dW-t3Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EgUe85Wn_B2j",
        "colab_type": "code",
        "outputId": "3018d814-0159-4e64-ded5-6ac71d4d2868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2346
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs=70\n",
        "\n",
        "valid_loss_min=np.Inf\n",
        "train_loss=0.0\n",
        "valid_loss=0.0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  model.train()\n",
        "  for batch_idx,(image,label) in enumerate(train_loader):\n",
        "    if(train_on_gpu):\n",
        "      image,label=image.cuda(),label.cuda()\n",
        "     \n",
        "    optimizer.zero_grad()\n",
        "    output=model(image)\n",
        "    loss=criterion(output,label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss+=loss.item()*image.size(0)\n",
        "    \n",
        "  model.eval()\n",
        "\n",
        " \n",
        "  \n",
        "  for batch_idx,(image,label) in enumerate(train_loader):\n",
        "    if(train_on_gpu):\n",
        "      image,label=image.cuda(),label.cuda()\n",
        "     \n",
        "    optimizer.zero_grad()\n",
        "    output=model(image)\n",
        "    loss=criterion(output,label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    valid_loss+=loss.item()*image.size(0)\n",
        "    \n",
        "    \n",
        "  train_loss=train_loss/len(train_loader.dataset)\n",
        "  valid_loss=valid_loss/len(valid_loader.dataset)\n",
        "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "  if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_augmented.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tTraining Loss: 0.925850 \tValidation Loss: 0.807239\n",
            "Validation loss decreased (inf --> 0.807239).  Saving model ...\n",
            "Epoch: 1 \tTraining Loss: 0.854013 \tValidation Loss: 0.730603\n",
            "Validation loss decreased (0.807239 --> 0.730603).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.793995 \tValidation Loss: 0.673116\n",
            "Validation loss decreased (0.730603 --> 0.673116).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.749019 \tValidation Loss: 0.623719\n",
            "Validation loss decreased (0.673116 --> 0.623719).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.711039 \tValidation Loss: 0.577649\n",
            "Validation loss decreased (0.623719 --> 0.577649).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.677349 \tValidation Loss: 0.542667\n",
            "Validation loss decreased (0.577649 --> 0.542667).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.641937 \tValidation Loss: 0.507791\n",
            "Validation loss decreased (0.542667 --> 0.507791).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.621284 \tValidation Loss: 0.478932\n",
            "Validation loss decreased (0.507791 --> 0.478932).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.595085 \tValidation Loss: 0.450688\n",
            "Validation loss decreased (0.478932 --> 0.450688).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.575445 \tValidation Loss: 0.423236\n",
            "Validation loss decreased (0.450688 --> 0.423236).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.564771 \tValidation Loss: 0.398168\n",
            "Validation loss decreased (0.423236 --> 0.398168).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.540639 \tValidation Loss: 0.382497\n",
            "Validation loss decreased (0.398168 --> 0.382497).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.528570 \tValidation Loss: 0.358429\n",
            "Validation loss decreased (0.382497 --> 0.358429).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.517030 \tValidation Loss: 0.339562\n",
            "Validation loss decreased (0.358429 --> 0.339562).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.505300 \tValidation Loss: 0.323052\n",
            "Validation loss decreased (0.339562 --> 0.323052).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.489604 \tValidation Loss: 0.305754\n",
            "Validation loss decreased (0.323052 --> 0.305754).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.485310 \tValidation Loss: 0.287979\n",
            "Validation loss decreased (0.305754 --> 0.287979).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.476268 \tValidation Loss: 0.274173\n",
            "Validation loss decreased (0.287979 --> 0.274173).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.466941 \tValidation Loss: 0.257380\n",
            "Validation loss decreased (0.274173 --> 0.257380).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.456889 \tValidation Loss: 0.247051\n",
            "Validation loss decreased (0.257380 --> 0.247051).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.447938 \tValidation Loss: 0.233042\n",
            "Validation loss decreased (0.247051 --> 0.233042).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.444051 \tValidation Loss: 0.223236\n",
            "Validation loss decreased (0.233042 --> 0.223236).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.439282 \tValidation Loss: 0.209936\n",
            "Validation loss decreased (0.223236 --> 0.209936).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.430587 \tValidation Loss: 0.199021\n",
            "Validation loss decreased (0.209936 --> 0.199021).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.420023 \tValidation Loss: 0.190653\n",
            "Validation loss decreased (0.199021 --> 0.190653).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.421502 \tValidation Loss: 0.182933\n",
            "Validation loss decreased (0.190653 --> 0.182933).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.408676 \tValidation Loss: 0.172756\n",
            "Validation loss decreased (0.182933 --> 0.172756).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.412238 \tValidation Loss: 0.165750\n",
            "Validation loss decreased (0.172756 --> 0.165750).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.404929 \tValidation Loss: 0.159111\n",
            "Validation loss decreased (0.165750 --> 0.159111).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.399638 \tValidation Loss: 0.150159\n",
            "Validation loss decreased (0.159111 --> 0.150159).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.396123 \tValidation Loss: 0.146397\n",
            "Validation loss decreased (0.150159 --> 0.146397).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 0.389629 \tValidation Loss: 0.142492\n",
            "Validation loss decreased (0.146397 --> 0.142492).  Saving model ...\n",
            "Epoch: 32 \tTraining Loss: 0.383485 \tValidation Loss: 0.134897\n",
            "Validation loss decreased (0.142492 --> 0.134897).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.380747 \tValidation Loss: 0.128960\n",
            "Validation loss decreased (0.134897 --> 0.128960).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 0.380747 \tValidation Loss: 0.127536\n",
            "Validation loss decreased (0.128960 --> 0.127536).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.376012 \tValidation Loss: 0.121118\n",
            "Validation loss decreased (0.127536 --> 0.121118).  Saving model ...\n",
            "Epoch: 36 \tTraining Loss: 0.375252 \tValidation Loss: 0.112367\n",
            "Validation loss decreased (0.121118 --> 0.112367).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 0.369047 \tValidation Loss: 0.111940\n",
            "Validation loss decreased (0.112367 --> 0.111940).  Saving model ...\n",
            "Epoch: 38 \tTraining Loss: 0.364708 \tValidation Loss: 0.107157\n",
            "Validation loss decreased (0.111940 --> 0.107157).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 0.359462 \tValidation Loss: 0.103218\n",
            "Validation loss decreased (0.107157 --> 0.103218).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.367549 \tValidation Loss: 0.101950\n",
            "Validation loss decreased (0.103218 --> 0.101950).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.357217 \tValidation Loss: 0.101266\n",
            "Validation loss decreased (0.101950 --> 0.101266).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.351249 \tValidation Loss: 0.096437\n",
            "Validation loss decreased (0.101266 --> 0.096437).  Saving model ...\n",
            "Epoch: 43 \tTraining Loss: 0.341093 \tValidation Loss: 0.091961\n",
            "Validation loss decreased (0.096437 --> 0.091961).  Saving model ...\n",
            "Epoch: 44 \tTraining Loss: 0.344781 \tValidation Loss: 0.092386\n",
            "Epoch: 45 \tTraining Loss: 0.341253 \tValidation Loss: 0.089194\n",
            "Validation loss decreased (0.091961 --> 0.089194).  Saving model ...\n",
            "Epoch: 46 \tTraining Loss: 0.343653 \tValidation Loss: 0.086922\n",
            "Validation loss decreased (0.089194 --> 0.086922).  Saving model ...\n",
            "Epoch: 47 \tTraining Loss: 0.340926 \tValidation Loss: 0.083980\n",
            "Validation loss decreased (0.086922 --> 0.083980).  Saving model ...\n",
            "Epoch: 48 \tTraining Loss: 0.340115 \tValidation Loss: 0.082774\n",
            "Validation loss decreased (0.083980 --> 0.082774).  Saving model ...\n",
            "Epoch: 49 \tTraining Loss: 0.333630 \tValidation Loss: 0.078714\n",
            "Validation loss decreased (0.082774 --> 0.078714).  Saving model ...\n",
            "Epoch: 50 \tTraining Loss: 0.335018 \tValidation Loss: 0.077640\n",
            "Validation loss decreased (0.078714 --> 0.077640).  Saving model ...\n",
            "Epoch: 51 \tTraining Loss: 0.334813 \tValidation Loss: 0.075475\n",
            "Validation loss decreased (0.077640 --> 0.075475).  Saving model ...\n",
            "Epoch: 52 \tTraining Loss: 0.328857 \tValidation Loss: 0.072384\n",
            "Validation loss decreased (0.075475 --> 0.072384).  Saving model ...\n",
            "Epoch: 53 \tTraining Loss: 0.333561 \tValidation Loss: 0.072502\n",
            "Epoch: 54 \tTraining Loss: 0.329071 \tValidation Loss: 0.070419\n",
            "Validation loss decreased (0.072384 --> 0.070419).  Saving model ...\n",
            "Epoch: 55 \tTraining Loss: 0.328126 \tValidation Loss: 0.070208\n",
            "Validation loss decreased (0.070419 --> 0.070208).  Saving model ...\n",
            "Epoch: 56 \tTraining Loss: 0.324124 \tValidation Loss: 0.068177\n",
            "Validation loss decreased (0.070208 --> 0.068177).  Saving model ...\n",
            "Epoch: 57 \tTraining Loss: 0.320362 \tValidation Loss: 0.067464\n",
            "Validation loss decreased (0.068177 --> 0.067464).  Saving model ...\n",
            "Epoch: 58 \tTraining Loss: 0.313688 \tValidation Loss: 0.063088\n",
            "Validation loss decreased (0.067464 --> 0.063088).  Saving model ...\n",
            "Epoch: 59 \tTraining Loss: 0.316930 \tValidation Loss: 0.062376\n",
            "Validation loss decreased (0.063088 --> 0.062376).  Saving model ...\n",
            "Epoch: 60 \tTraining Loss: 0.312987 \tValidation Loss: 0.061774\n",
            "Validation loss decreased (0.062376 --> 0.061774).  Saving model ...\n",
            "Epoch: 61 \tTraining Loss: 0.313057 \tValidation Loss: 0.061500\n",
            "Validation loss decreased (0.061774 --> 0.061500).  Saving model ...\n",
            "Epoch: 62 \tTraining Loss: 0.314941 \tValidation Loss: 0.060489\n",
            "Validation loss decreased (0.061500 --> 0.060489).  Saving model ...\n",
            "Epoch: 63 \tTraining Loss: 0.312299 \tValidation Loss: 0.057953\n",
            "Validation loss decreased (0.060489 --> 0.057953).  Saving model ...\n",
            "Epoch: 64 \tTraining Loss: 0.304079 \tValidation Loss: 0.058455\n",
            "Epoch: 65 \tTraining Loss: 0.311980 \tValidation Loss: 0.057254\n",
            "Validation loss decreased (0.057953 --> 0.057254).  Saving model ...\n",
            "Epoch: 66 \tTraining Loss: 0.309062 \tValidation Loss: 0.056127\n",
            "Validation loss decreased (0.057254 --> 0.056127).  Saving model ...\n",
            "Epoch: 67 \tTraining Loss: 0.305161 \tValidation Loss: 0.055609\n",
            "Validation loss decreased (0.056127 --> 0.055609).  Saving model ...\n",
            "Epoch: 68 \tTraining Loss: 0.298958 \tValidation Loss: 0.053679\n",
            "Validation loss decreased (0.055609 --> 0.053679).  Saving model ...\n",
            "Epoch: 69 \tTraining Loss: 0.305862 \tValidation Loss: 0.051646\n",
            "Validation loss decreased (0.053679 --> 0.051646).  Saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l2KuLY7_ON8E",
        "colab_type": "code",
        "outputId": "bd9af520-c2d9-4899-dcc3-8adede6976aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model_augmented.pt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-13f87a119d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_augmented.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}